{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import pickle\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "# This section gathers data using tweepy package. The file runs for 10 minutes.\n",
    "# Additionally, I originally had this section running as an automated task \n",
    "# every hour on the hour.\n",
    "#-------------------------------------------------------------------------------\n",
    "\n",
    "d = datetime.now()\n",
    "def create_api():\n",
    "    con_key = ''\n",
    "    con_secret = ''\n",
    "    acc_token = ''\n",
    "    acc_secret = ''\n",
    "\n",
    "    auth = tweepy.OAuthHandler(consumer_key=con_key, consumer_secret=con_secret)\n",
    "    auth.set_access_token(acc_token, acc_secret)\n",
    "\n",
    "    return tweepy.API(auth)\n",
    "\n",
    "\n",
    "class ListStreamListener(tweepy.StreamListener):\n",
    "    '''This class overides tweepy.StreamListener'''\n",
    "    def __init__(self, time_limit=600):\n",
    "        '''The class is initialized with a start time and time limit'''\n",
    "        self.start_time = time.time()\n",
    "        self.limit = time_limit\n",
    "        super(ListStreamListener, self).__init__()\n",
    "        self.my_tweets = []\n",
    "\n",
    "    def on_status(self, status):\n",
    "        '''As long as time limit is not reached, data will be collected.\n",
    "        At the point the time limit is reached, the data will be saved \n",
    "        as a pickle file\n",
    "        '''\n",
    "        #I store hashtags, urls, and user mentions as lists\n",
    "        ent_ht = status.entities.get('hashtags')\n",
    "        ht = [item.get('text') for item in ent_ht] \n",
    "        ent_url = status.entities.get('urls')\n",
    "        url = [item.get('expanded_url')for item in ent_url]\n",
    "        ent_um = status.entities.get('user_mentions')\n",
    "        um = [item.get('screen_name') for item in ent_um]\n",
    "        if (time.time() - self.start_time) < self.limit:\n",
    "            self.my_tweets.append((status.id_str,\n",
    "                                   status.created_at, \n",
    "                                   status.user.utc_offset,\n",
    "                                   status.retweet_count,\n",
    "                                   status.in_reply_to_screen_name,\n",
    "                                   status.source,\n",
    "                                   status.place.full_name,\n",
    "                                   status.user.location,\n",
    "                                   ht,\n",
    "                                   url,\n",
    "                                   um,\n",
    "                                   status.text))\n",
    "        else:\n",
    "            with open('C:\\\\tools\\\\dat\\\\tweet_{0}.pickle'.format(d.strftime('%Y%m%d%H%M%S')), 'wb') as f:\n",
    "                pickle.dump(my_stream_listener.my_tweets, f)\n",
    "\n",
    "            return False\n",
    "    \n",
    "    def on_error(self, status_code):\n",
    "        if status_code == 420:\n",
    "            return False\n",
    "\n",
    "api = create_api()\n",
    "my_stream_listener = ListStreamListener(time_limit=600)\n",
    "my_stream = tweepy.Stream(auth = api.auth, listener=my_stream_listener)\n",
    "#The geographic coordinates are for the Twin Cities.\n",
    "my_stream.filter(locations=[-93.4947038,44.8455126,-92.8447074,45.0582123])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "# This section creates and saves the data frame for further analysis in R.  \n",
    "# There are numerous helper methods for natural language processing.\n",
    "#-------------------------------------------------------------------------------\n",
    "\n",
    "def load_pickle(file):\n",
    "    with open('dat/{0}'.format(file), 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def create_df():\n",
    "    utc_diff = 6\n",
    "    cols=['id', \n",
    "          'created_at', \n",
    "          'utc_offset', \n",
    "          'retweet_count', \n",
    "          'reply_screen_name', \n",
    "          'source', \n",
    "          'place_full_name', \n",
    "          'user_location', \n",
    "          'hashtags', \n",
    "          'urls',\n",
    "          'user_mentions',\n",
    "          'text']\n",
    "\n",
    "    my_tweets = []\n",
    "    for file in os.listdir('dat'):\n",
    "        my_tweets = my_tweets + load_pickle(file)\n",
    "    df = pd.DataFrame(my_tweets, columns=cols)\n",
    "    # I remove the hashtags, urls, and user mentions from the text because\n",
    "    # it is contained in another file\n",
    "    df['text_clean'] = df.text.replace('(@|#|http(s)?://)[^ ]+', '', regex=True)\n",
    "    df['hour'] = df.created_at.dt.hour\n",
    "    df['ToD'] = np.NaN\n",
    "    df.loc[df.hour.ge(5+utc_diff) & df.hour.le(10+utc_diff), 'ToD'] = 'morning'\n",
    "    df.loc[df.hour.ge(16+utc_diff) & df.hour.le(21+utc_diff), 'ToD'] = 'night'\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_text(iterable):\n",
    "    text = \"\".join(iterable)\n",
    "    return text.lower()\n",
    "\n",
    "\n",
    "def create_tokens(text):\n",
    "    return nltk.word_tokenize(text)\n",
    "\n",
    "\n",
    "def remove_punc(text):\n",
    "    pun_dict = {ord(punc):None for punc in string.punctuation}\n",
    "    text_clean = text.translate(pun_dict)\n",
    "    return text_clean\n",
    "\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    tokens_clean = [t for t in tokens if t not in stopwords]\n",
    "    return tokens_clean\n",
    "    \n",
    "\n",
    "def lemmatize_tokens(tokens):\n",
    "    #lemmatizer\n",
    "    wnl = nltk.WordNetLemmatizer()\n",
    "    return [wnl.lemmatize(t) for t in tokens]\n",
    "\n",
    "    \n",
    "# def stem_tokens(tokens):\n",
    "    # #Stemmer\n",
    "    # porter = nltk.PorterStemmer()\n",
    "    # return [porter.stem(t) for t in tokens]\n",
    "\n",
    "\n",
    "def create_pos(tokens):\n",
    "    return nltk.pos_tag(tokens)\n",
    "\n",
    "\n",
    "def create_fdist(tokens):\n",
    "    return nltk.FreqDist(tokens)\n",
    "\n",
    "\n",
    "def get_pos(pos, part='NN'):\n",
    "    nouns = [word[0] for word in pos if word[1] == part]\n",
    "    return nouns\n",
    "\n",
    "\n",
    "def create_textobj(tokens):\n",
    "    return nltk.Text(tokens)\n",
    "\n",
    "\n",
    "def add_sentiment(df):\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    sentiment = pd.DataFrame([sid.polarity_scores(s) for s in df.text_clean])\n",
    "    return pd.concat([df,sentiment],axis=1)\n",
    "\n",
    "\n",
    "def create_clean_tokens(df):\n",
    "    text = remove_punc(create_text(df.text_clean))\n",
    "    tokens = create_tokens(text)\n",
    "    return lemmatize_tokens(remove_stopwords(tokens))\n",
    "\n",
    "\n",
    "def create_wordcloud(df, part='NN'):\n",
    "    ''' can be VB or NN '''\n",
    "    tokens_clean = create_clean_tokens(df)\n",
    "    if part:\n",
    "        pos = create_pos(tokens_clean)\n",
    "        part_list = get_pos(pos, part='NN')\n",
    "        return pd.DataFrame(part_list, columns=[part])\n",
    "    return pd.DataFrame(tokens_clean, columns=['tokens'])\n",
    "\n",
    "\n",
    "def save_df(df, name):\n",
    "    df.to_csv('{0}.csv'.format(name), index=False)\n",
    "    print('I successfully saved the {}!'.format(name))\n",
    "\n",
    "\n",
    "def get_morn_and_night(df):\n",
    "    return df.loc[df.ToD.eq('morning') | df.ToD.eq('night'), :]\n",
    "\n",
    "\n",
    "def get_morn(df):\n",
    "    return df.loc[df.ToD.eq('morning'), :]\n",
    "\n",
    "\n",
    "def get_night(df):\n",
    "    return df.loc[df.ToD.eq('night'), :]\n",
    "\n",
    "#The following loads and saves the dataframes \n",
    "df = add_sentiment(create_df())\n",
    "\n",
    "df_mn = get_morn_and_night(df)\n",
    "save_df(df_mn, 'data_sentiment')\n",
    "\n",
    "wc_m = create_wordcloud(get_morn(df))\n",
    "wc_n = create_wordcloud(get_night(df))\n",
    "\n",
    "save_df(wc_m, 'data_wc_morn')\n",
    "save_df(wc_n, 'data_wc_night')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
